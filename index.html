<!DOCTYPE html>
<html lang="en">
<head>
Â  Â  <meta charset="UTF-8">
Â  Â  <meta name="viewport" content="width=device-width, initial-scale=1.0">
Â  Â  <title>MoodFlow: Real-Time Mood Detection</title>
Â  Â  <!-- Load Tailwind CSS -->
Â  Â  <script src="https://cdn.tailwindcss.com"></script>
Â  Â  <!-- Load Face-API.js -->
Â  Â  <script src="https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/dist/face-api.min.js"></script>

Â  Â  <style>
Â  Â  Â  Â  /* Custom styles for a clean, eye-catching look */
Â  Â  Â  Â  @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap');
Â  Â  Â  Â  body {
Â  Â  Â  Â  Â  Â  font-family: 'Inter', sans-serif;
Â  Â  Â  Â  }
Â  Â  Â  Â  #video, #canvas {
Â  Â  Â  Â  Â  Â  position: absolute;
Â  Â  Â  Â  Â  Â  top: 0;
Â  Â  Â  Â  Â  Â  left: 0;
Â  Â  Â  Â  }
Â  Â  Â  Â  .video-container {
Â  Â  Â  Â  Â  Â  position: relative;
Â  Â  Â  Â  Â  Â  width: 100%;
Â  Â  Â  Â  Â  Â  max-width: 400px;
Â  Â  Â  Â  Â  Â  height: 300px;
Â  Â  Â  Â  Â  Â  overflow: hidden;
Â  Â  Â  Â  Â  Â  border-radius: 1.5rem; /* Large rounded corners */
Â  Â  Â  Â  Â  Â  box-shadow: 0 10px 30px rgba(0, 0, 0, 0.1); /* Soft shadow */
Â  Â  Â  Â  }
Â  Â  Â  Â  /* Spinner CSS for API loading */
Â  Â  Â  Â  .spinner {
Â  Â  Â  Â  Â  Â  border: 4px solid rgba(0, 0, 0, 0.1);
Â  Â  Â  Â  Â  Â  width: 24px;
Â  Â  Â  Â  Â  Â  height: 24px;
Â  Â  Â  Â  Â  Â  border-radius: 50%;
Â  Â  Â  Â  Â  Â  border-left-color: #4F46E5;
Â  Â  Â  Â  Â  Â  animation: spin 1s ease infinite;
Â  Â  Â  Â  Â  Â  display: inline-block;
Â  Â  Â  Â  Â  Â  vertical-align: middle;
Â  Â  Â  Â  }
Â  Â  Â  Â  @keyframes spin {
Â  Â  Â  Â  Â  Â  0% { transform: rotate(0deg); }
Â  Â  Â  Â  Â  Â  100% { transform: rotate(360deg); }
Â  Â  Â  Â  }
Â  Â  </style>
</head>
<body class="min-h-screen flex flex-col bg-[#f7f9fc]">

Â  Â  <!-- Header / Navigation Bar -->
Â  Â  <header class="p-4 bg-white shadow-md sticky top-0 z-10">
Â  Â  Â  Â  <div class="max-w-7xl mx-auto flex justify-between items-center">
Â  Â  Â  Â  Â  Â  <h1 class="text-2xl font-extrabold text-indigo-600 tracking-wider">
Â  Â  Â  Â  Â  Â  Â  Â  <span class="text-green-500">M</span>ood<span class="text-green-500">F</span>low
Â  Â  Â  Â  Â  Â  </h1>
Â  Â  Â  Â  Â  Â  <nav>
Â  Â  Â  Â  Â  Â  Â  Â  <a href="#" class="text-gray-600 hover:text-indigo-600 transition duration-150 text-sm md:text-base">AI Powered</a>
Â  Â  Â  Â  Â  Â  </nav>
Â  Â  Â  Â  </div>
Â  Â  </header>

Â  Â  <!-- Main Content Area -->
Â  Â  <main class="flex-grow flex items-center justify-center p-4">
Â  Â  Â  Â  <!-- Main Content Card -->
Â  Â  Â  Â  <div class="bg-white p-8 md:p-12 rounded-3xl shadow-2xl max-w-4xl w-full flex flex-col md:flex-row gap-10">

Â  Â  Â  Â  Â  Â  <!-- Camera Feed & Canvas Section -->
Â  Â  Â  Â  Â  Â  <div class="flex-shrink-0">
Â  Â  Â  Â  Â  Â  Â  Â  <h2 class="text-2xl font-bold text-gray-800 mb-4 flex items-center">
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  <svg xmlns="http://www.w3.org/2000/svg" class="h-6 w-6 mr-2 text-indigo-500" fill="none" viewBox="0 0 24 24" stroke="currentColor">
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M3 9a2 2 0 012-2h.93a2 2 0 001.664-.89l.812-1.218A2 2 0 0110.07 4h3.86a2 2 0 011.664.89l.812 1.218A2 2 0 0018.07 7H19a2 2 0 012 2v9a2 2 0 01-2 2H5a2 2 0 01-2-2V9z" />
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M15 13a3 3 0 11-6 0 3 3 0 016 0z" />
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  </svg>
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Your Camera
Â  Â  Â  Â  Â  Â  Â  Â  </h2>
Â  Â  Â  Â  Â  Â  Â  Â  <div class="video-container">
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  <video id="video" width="400" height="300" autoplay muted></video>
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  <canvas id="canvas" width="400" height="300"></canvas>
Â  Â  Â  Â  Â  Â  Â  Â  </div>
Â  Â  Â  Â  Â  Â  Â  Â  <p id="loading-status" class="mt-4 text-sm text-yellow-600 font-semibold">Loading AI Models... Please Wait.</p>
Â  Â  Â  Â  Â  Â  </div>

Â  Â  Â  Â  Â  Â  <!-- Result & Suggestion Section -->
Â  Â  Â  Â  Â  Â  <div class="flex-grow">
Â  Â  Â  Â  Â  Â  Â  Â  <h2 class="text-2xl font-bold text-gray-800 mb-6 flex items-center">
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  <svg xmlns="http://www.w3.org/2000/svg" class="h-6 w-6 mr-2 text-green-500" fill="none" viewBox="0 0 24 24" stroke="currentColor">
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12l2 2 4-4m5.618-4.016A11.955 11.955 0 0112 2.944a11.955 11.955 0 01-8.618 4.016M12 2.944v14.128" />
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 2.944a11.955 11.955 0 014.577 15.011s.5-1.5.5-2.5m-5.077 2.5a11.955 11.955 0 01-4.577-15.011S7 5 7 6a2.5 2.5 0 015 0z" />
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  </svg>
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Mood Analysis
Â  Â  Â  Â  Â  Â  Â  Â  </h2>

Â  Â  Â  Â  Â  Â  Â  Â  <!-- Detected Mood Display -->
Â  Â  Â  Â  Â  Â  Â  Â  <div class="mb-6 p-4 bg-indigo-50 rounded-xl border border-indigo-200">
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  <p class="text-lg font-semibold text-indigo-700">Current Mood:</p>
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  <p id="mood-output" class="text-4xl font-extrabold text-indigo-900 mt-1">Awaiting Face...</p>
Â  Â  Â  Â  Â  Â  Â  Â  </div>

Â  Â  Â  Â  Â  Â  Â  Â  <!-- Activity Suggestion Area -->
Â  Â  Â  Â  Â  Â  Â  Â  <div class="p-6 bg-green-50 rounded-xl border border-green-200">
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  <p class="text-xl font-bold text-green-700 mb-3 flex items-center">
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-2" viewBox="0 0 20 20" fill="currentColor">
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  <path fill-rule="evenodd" d="M10 18a8 8 0 100-16 8 8 0 000 16zm1-12a1 1 0 10-2 0v4a1 1 0 00.293.707l3 3a1 1 0 001.414-1.414L11 9.586V6z" clip-rule="evenodd" />
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  </svg>
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  AI-Powered Suggestion
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  <span id="suggestion-loading" class="spinner ml-3 hidden"></span>
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  </p>
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  <p id="suggestion-content" class="text-gray-600 mb-4">
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Position your face clearly in front of the camera to begin the AI analysis.
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  </p>

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  <p class="text-sm font-bold text-yellow-700 mt-4">A Deeper Question:</p>
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  <p id="question-content" class="text-gray-800 italic">
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  What are you hoping to discover today?
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  </p>

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  <div id="sources-output" class="mt-4 text-xs text-gray-500"></div>
Â  Â  Â  Â  Â  Â  Â  Â  </div>

Â  Â  Â  Â  Â  Â  Â  Â  <!-- Error/Instruction Message -->
Â  Â  Â  Â  Â  Â  Â  Â  <p id="error-message" class="text-red-500 mt-4 text-center hidden">Camera access denied or failed to load. Please ensure permissions are granted.</p>
Â  Â  Â  Â  Â  Â  </div>
Â  Â  Â  Â  </div>
Â  Â  </main>
Â  Â Â 
Â  Â  <!-- Footer -->
Â  Â  <footer class="mt-auto p-4 text-center text-sm text-gray-500 border-t border-gray-200 bg-white">
Â  Â  Â  Â  <p>&copy; 2024 MoodFlow. Real-time emotion detection powered by Face-API.js and Gemini AI.</p>
Â  Â  </footer>

Â  Â  <!-- JavaScript Logic -->
Â  Â  <script type="module">
Â  Â  Â  Â  // --- Firebase Boilerplate (Required by Canvas environment for auth/storage) ---
Â  Â  Â  Â  import { initializeApp } from "https://www.gstatic.com/firebasejs/11.6.1/firebase-app.js";
Â  Â  Â  Â  import { getAuth, signInAnonymously, signInWithCustomToken } from "https://www.gstatic.com/firebasejs/11.6.1/firebase-auth.js";
Â  Â  Â  Â  import { getFirestore } from "https://www.gstatic.com/firebasejs/11.6.1/firebase-firestore.js";
Â  Â  Â  Â  import { setLogLevel } from "https://www.gstatic.com/firebasejs/11.6.1/firebase-firestore.js";

Â  Â  Â  Â  // Set Firebase Log Level to Debug
Â  Â  Â  Â  setLogLevel('Debug');

Â  Â  Â  Â  const firebaseConfig = JSON.parse(typeof __firebase_config !== 'undefined' ? __firebase_config : '{}');
Â  Â  Â  Â  const __initial_auth_token = typeof __initial_auth_token !== 'undefined' ? __initial_auth_token : undefined;

Â  Â  Â  Â  let app, db, auth;

Â  Â  Â  Â  // Initialize and authenticate
Â  Â  Â  Â  try {
Â  Â  Â  Â  Â  Â  if (Object.keys(firebaseConfig).length > 0) {
Â  Â  Â  Â  Â  Â  Â  Â  app = initializeApp(firebaseConfig);
Â  Â  Â  Â  Â  Â  Â  Â  db = getFirestore(app);
Â  Â  Â  Â  Â  Â  Â  Â  auth = getAuth(app);

Â  Â  Â  Â  Â  Â  Â  Â  if (__initial_auth_token) {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  await signInWithCustomToken(auth, __initial_auth_token);
Â  Â  Â  Â  Â  Â  Â  Â  } else {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  await signInAnonymously(auth);
Â  Â  Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  Â  Â  Â  Â  console.log("Firebase initialized and user signed in.");
Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  } catch (error) {
Â  Â  Â  Â  Â  Â  console.error("Firebase initialization failed:", error);
Â  Â  Â  Â  }
Â  Â  Â  Â  // --- End Firebase Boilerplate ---

Â  Â  Â  Â  const video = document.getElementById('video');
Â  Â  Â  Â  const canvas = document.getElementById('canvas');
Â  Â  Â  Â  const moodOutput = document.getElementById('mood-output');
Â  Â  Â  Â  const suggestionContent = document.getElementById('suggestion-content');
Â  Â  Â  Â  const questionContent = document.getElementById('question-content');
Â  Â  Â  Â  const suggestionLoading = document.getElementById('suggestion-loading');
Â  Â  Â  Â  const loadingStatus = document.getElementById('loading-status');
Â  Â  Â  Â  const errorMessage = document.getElementById('error-message');
Â  Â  Â  Â  const sourcesOutput = document.getElementById('sources-output');

Â  Â  Â  Â  let modelsLoaded = false;
Â  Â  Â  Â  let isDetecting = false;
Â  Â  Â  Â  let lastSuggestedMood = null;
Â  Â  Â  Â  let lastApiCallTime = 0;
Â  Â  Â  Â  const API_CALL_DELAY = 5000; // Throttle API calls to max once every 5 seconds

Â  Â  Â  Â  // Mood definitions with prompt structure
Â  Â  Â  Â  const MOODS = {
Â  Â  Â  Â  Â  Â  'happy': { emoji: 'ğŸ˜„ Happy', color: 'text-green-600', prompt: "Generate a fun, specific activity idea or a positive thought to amplify and maintain this positive mood, using up-to-date web suggestions." },
Â  Â  Â  Â  Â  Â  'neutral': { emoji: 'ğŸ˜ Neutral', color: 'text-blue-600', prompt: "Generate a gentle, motivating suggestion to introduce slight positive change, like a simple productivity tip or an interesting short read." },
Â  Â  Â  Â  Â  Â  'sad': { emoji: 'ğŸ˜” Sad', color: 'text-indigo-600', prompt: "Generate a compassionate, self-care activity idea to gently process or shift this mood, such as a guided reflection or a comfort activity." },
Â  Â  Â  Â  Â  Â  'angry': { emoji: 'ğŸ˜¡ Anger', color: 'text-red-600', prompt: "Generate a productive, healthy way to channel or release this energy, such as a physical activity, a focused task, or a writing exercise." },
Â  Â  Â  Â  Â  Â  'fearful': { emoji: 'ğŸ˜¨ Anxious', color: 'text-yellow-600', prompt: "Generate a calming, grounding technique or a simple step-by-step action to regain a sense of control and clarity, using search for relevant tips." },
Â  Â  Â  Â  Â  Â  'disgusted': { emoji: 'ğŸ˜® Focused', color: 'text-purple-600', prompt: "Generate a short break idea or a task management technique to refresh focus and maintain efficiency." }
Â  Â  Â  Â  };

Â  Â  Â  Â  // --- GEMINI API INTEGRATION ---
Â  Â  Â  Â Â 
Â  Â  Â  Â  /**
Â  Â  Â  Â  Â * Generic fetch wrapper with exponential backoff for API calls.
Â  Â  Â  Â  Â */
Â  Â  Â  Â  async function fetchWithBackoff(url, options, retries = 3) {
Â  Â  Â  Â  Â  Â  for (let i = 0; i < retries; i++) {
Â  Â  Â  Â  Â  Â  Â  Â  try {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  const response = await fetch(url, options);
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if (!response.ok) {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  // Throw error for non-2xx responses
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  throw new Error(`HTTP error! status: ${response.status}`);
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return response;
Â  Â  Â  Â  Â  Â  Â  Â  } catch (error) {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if (i === retries - 1) throw error; // Re-throw on last attempt
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  const delay = Math.pow(2, i) * 1000 + Math.random() * 1000;
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  await new Promise(resolve => setTimeout(resolve, delay));
Â  Â  Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  }

Â  Â  Â  Â  /**
Â  Â  Â  Â  Â * Calls the Gemini API to generate a suggestion and question based on mood.
Â  Â  Â  Â  Â */
Â  Â  Â  Â  async function generateSuggestion(mood) {
Â  Â  Â  Â  Â  Â  const moodData = MOODS[mood];
Â  Â  Â  Â  Â  Â  const systemPrompt = `You are a friendly, encouraging life coach and mental wellness assistant. Your task is to provide a single, actionable suggestion and a single reflective question based on the user's current mood. Structure your entire response into two distinct, labeled parts: "Suggestion:" followed by the activity, and "Question:" followed by the reflective query.`;
Â  Â  Â  Â  Â  Â  const userQuery = `The user's current mood is "${moodData.emoji}". ${moodData.prompt} After the suggestion, generate one concise, reflective question aimed at self-discovery or grounding.`;

Â  Â  Â  Â  Â  Â  const apiKey = ""; // Canvas environment provides this if needed
Â  Â  Â  Â  Â  Â  const apiUrl = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-09-2025:generateContent?key=${apiKey}`;
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  const payload = {
Â  Â  Â  Â  Â  Â  Â  Â  contents: [{ parts: [{ text: userQuery }] }],
Â  Â  Â  Â  Â  Â  Â  Â  tools: [{ "google_search": {} }],
Â  Â  Â  Â  Â  Â  Â  Â  systemInstruction: { parts: [{ text: systemPrompt }] },
Â  Â  Â  Â  Â  Â  };

Â  Â  Â  Â  Â  Â  suggestionLoading.classList.remove('hidden');
Â  Â  Â  Â  Â  Â  sourcesOutput.innerHTML = '';
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  try {
Â  Â  Â  Â  Â  Â  Â  Â  const response = await fetchWithBackoff(apiUrl, {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  method: 'POST',
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  headers: { 'Content-Type': 'application/json' },
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  body: JSON.stringify(payload)
Â  Â  Â  Â  Â  Â  Â  Â  });
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  const result = await response.json();
Â  Â  Â  Â  Â  Â  Â  Â  const candidate = result.candidates?.[0];

Â  Â  Â  Â  Â  Â  Â  Â  if (candidate && candidate.content?.parts?.[0]?.text) {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  const text = candidate.content.parts[0].text;
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  let sources = [];
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  // Extract Grounding Sources (Citations)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  const groundingMetadata = candidate.groundingMetadata;
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if (groundingMetadata && groundingMetadata.groundingAttributions) {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  sources = groundingMetadata.groundingAttributions
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  .map(attr => ({
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  uri: attr.web?.uri,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  title: attr.web?.title,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  }))
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  .filter(source => source.uri && source.title);
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  }

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  // Parse the generated text into Suggestion and Question
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  const parts = text.split(/Suggestion:|Question:/i).map(s => s.trim()).filter(s => s.length > 0);
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if (parts.length >= 2) {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  // The prompt asks for "Suggestion:" then "Question:"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  suggestionContent.textContent = parts[0].trim().replace(/\*|#|\n/g, ''); // Clean markdown
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  questionContent.textContent = parts[1].trim().replace(/\*|#|\n/g, ''); // Clean markdown
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  } else if (parts.length === 1) {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â // Fallback for unexpected format
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  suggestionContent.textContent = parts[0].trim().replace(/\*|#|\n/g, '');
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  questionContent.textContent = 'Reflect on this: How does this feeling serve you today?';
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  } else {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  throw new Error("AI output was empty or unparseable.");
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  }

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  // Display Sources
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if (sources.length > 0) {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  const uniqueSources = Array.from(new Set(sources.map(s => s.uri)))
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  .map(uri => sources.find(s => s.uri === uri));
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  sourcesOutput.innerHTML = 'Sources: ' + uniqueSources.map((s, i) =>Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  `<a href="${s.uri}" target="_blank" class="text-indigo-500 hover:underline">${s.title.substring(0, 30)}...</a>`
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ).join(', ');
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  }

Â  Â  Â  Â  Â  Â  Â  Â  } else {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  throw new Error("AI did not return content.");
Â  Â  Â  Â  Â  Â  Â  Â  }

Â  Â  Â  Â  Â  Â  } catch (e) {
Â  Â  Â  Â  Â  Â  Â  Â  console.error("AI suggestion failed:", e);
Â  Â  Â  Â  Â  Â  Â  Â  suggestionContent.textContent = `Sorry, the AI helper is unavailable right now. Please try reloading or check the console for details.`;
Â  Â  Â  Â  Â  Â  Â  Â  questionContent.textContent = `Try a simple technique: take a 30-second break.`;
Â  Â  Â  Â  Â  Â  } finally {
Â  Â  Â  Â  Â  Â  Â  Â  suggestionLoading.classList.add('hidden');
Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  }

Â  Â  Â  Â  // --- FACE-API LOGIC ---

Â  Â  Â  Â  // Custom function to find the most likely mood (mapping 'fearful' as 'anxious')
Â  Â  Â  Â  function getBestMood(expressions) {
Â  Â  Â  Â  Â  Â  if (!expressions) return null;

Â  Â  Â  Â  Â  Â  const moodKeys = ['happy', 'neutral', 'sad', 'angry', 'fearful', 'disgusted'];
Â  Â  Â  Â  Â  Â  let bestMood = 'neutral';
Â  Â  Â  Â  Â  Â  let maxConfidence = 0.5; // Start with a decent baseline
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  // Look for the most confident expression
Â  Â  Â  Â  Â  Â  for (const key of moodKeys) {
Â  Â  Â  Â  Â  Â  Â  Â  if (expressions[key] > maxConfidence) {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  maxConfidence = expressions[key];
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  bestMood = key;
Â  Â  Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  // If the max confidence is too low, default to 'neutral'
Â  Â  Â  Â  Â  Â  if (maxConfidence < 0.25) {
Â  Â  Â  Â  Â  Â  Â  Â  return 'neutral';
Â  Â  Â  Â  Â  Â  }

Â  Â  Â  Â  Â  Â  return bestMood;
Â  Â  Â  Â  }

Â  Â  Â  Â  // 1. Load the Models
Â  Â  Â  Â  async function loadModels() {
Â  Â  Â  Â  Â  Â  const MODEL_URL = 'https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/model';
Â  Â  Â  Â  Â  Â  try {
Â  Â  Â  Â  Â  Â  Â  Â  await faceapi.nets.tinyFaceDetector.load(MODEL_URL);
Â  Â  Â  Â  Â  Â  Â  Â  await faceapi.nets.faceLandmark68Net.load(MODEL_URL);
Â  Â  Â  Â  Â  Â  Â  Â  await faceapi.nets.faceExpressionNet.load(MODEL_URL);
Â  Â  Â  Â  Â  Â  Â  Â  modelsLoaded = true;
Â  Â  Â  Â  Â  Â  Â  Â  loadingStatus.textContent = 'AI Models loaded! Starting camera stream...';
Â  Â  Â  Â  Â  Â  Â  Â  startVideo();
Â  Â  Â  Â  Â  Â  } catch (e) {
Â  Â  Â  Â  Â  Â  Â  Â  loadingStatus.textContent = 'Error loading face models. Check your network.';
Â  Â  Â  Â  Â  Â  Â  Â  console.error('Model load error:', e);
Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  }

Â  Â  Â  Â  // 2. Start Video Stream
Â  Â  Â  Â  function startVideo() {
Â  Â  Â  Â  Â  Â  navigator.mediaDevices.getUserMedia({ video: { width: 400, height: 300 } })
Â  Â  Â  Â  Â  Â  Â  Â  .then(stream => {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  video.srcObject = stream;
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  loadingStatus.style.display = 'none';
Â  Â  Â  Â  Â  Â  Â  Â  })
Â  Â  Â  Â  Â  Â  Â  Â  .catch(err => {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  console.error("Camera access error:", err);
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  loadingStatus.textContent = 'Access Denied.';
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  errorMessage.textContent = 'Camera access denied or failed to load. Please ensure permissions are granted in your browser settings.';
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  errorMessage.classList.remove('hidden');
Â  Â  Â  Â  Â  Â  Â  Â  });
Â  Â  Â  Â  }

Â  Â  Â  Â  // 3. Start Detection Loop
Â  Â  Â  Â  video.addEventListener('play', () => {
Â  Â  Â  Â  Â  Â  if (isDetecting) return;

Â  Â  Â  Â  Â  Â  const displaySize = { width: video.width, height: video.height };
Â  Â  Â  Â  Â  Â  faceapi.matchDimensions(canvas, displaySize);

Â  Â  Â  Â  Â  Â  // Set interval for detection
Â  Â  Â  Â  Â  Â  isDetecting = true;
Â  Â  Â  Â  Â  Â  setInterval(async () => {
Â  Â  Â  Â  Â  Â  Â  Â  const detections = await faceapi.detectAllFaces(video, new faceapi.TinyFaceDetectorOptions({ inputSize: 256 }))
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  .withFaceLandmarks()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  .withFaceExpressions();

Â  Â  Â  Â  Â  Â  Â  Â  // Clear the canvas from previous drawing
Â  Â  Â  Â  Â  Â  Â  Â  const ctx = canvas.getContext('2d');
Â  Â  Â  Â  Â  Â  Â  Â  ctx.clearRect(0, 0, canvas.width, canvas.height);

Â  Â  Â  Â  Â  Â  Â  Â  if (detections.length > 0) {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  const resizedDetections = faceapi.resizeResults(detections, displaySize);
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  faceapi.draw.drawDetections(canvas, resizedDetections, { boxColor: '#4F46E5', lineWidth: 2 });

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  const expressions = resizedDetections[0].expressions;
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  const currentMoodKey = getBestMood(expressions);
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  const moodData = MOODS[currentMoodKey];

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  // Update UI
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  moodOutput.innerHTML = `${moodData.emoji}`;
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  moodOutput.className = `text-4xl font-extrabold mt-1 ${moodData.color}`;
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  // Call AI Suggestion only if mood changes or enough time has passed
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  const currentTime = Date.now();
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if (currentMoodKey !== lastSuggestedMood || (currentTime - lastApiCallTime > API_CALL_DELAY && suggestionLoading.classList.contains('hidden'))) {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  lastSuggestedMood = currentMoodKey;
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  lastApiCallTime = currentTime;
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  generateSuggestion(currentMoodKey);
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  } else {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  // No face detected
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  moodOutput.textContent = 'Awaiting Face...';
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  moodOutput.className = 'text-4xl font-extrabold text-indigo-900 mt-1';
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  suggestionContent.textContent = 'Please move closer or ensure your face is well-lit and fully visible to start the AI analysis.';
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  questionContent.textContent = 'What are you hoping to discover today?';
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  sourcesOutput.innerHTML = '';
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  lastSuggestedMood = null; // Reset suggestion state
Â  Â  Â  Â  Â  Â  Â  Â  }

Â  Â  Â  Â  Â  Â  }, 500); // Run face detection every 500ms
Â  Â  Â  Â  });

Â  Â  Â  Â  // Start the process
Â  Â  Â  Â  window.onload = loadModels;

Â  Â  </script>
</body>
</html>
