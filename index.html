import cv2
from deepface import DeepFace
import numpy as np

# --- Configuration ---
# We will map the 'surprise' emotion from deepface to 'confused' as requested.
MOOD_MAP = {
    'angry': 'Anger',
    'disgust': 'Disgust',
    'fear': 'Fear',
    'happy': 'Happy',
    'sad': 'Sad',
    'surprise': 'Confused', # Mapping surprise to confused
    'neutral': 'Neutral'
}
# Colors for the bounding box and text
BOX_COLOR = (0, 255, 255) # Yellow
TEXT_COLOR = (255, 255, 255) # White
FONT = cv2.FONT_HERSHEY_SIMPLEX

# --- Load Models ---
# Load OpenCV's pre-trained face detector (Haar Cascade)
# This is faster for just finding the face boundaries.
try:
    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
except Exception as e:
    print(f"Error loading Haar Cascade model: {e}")
    print("Please ensure OpenCV is installed correctly.")
    exit()

# Pre-load the DeepFace model to avoid delay on first frame
# This will use the default model (VGG-Face) and backend.
print("Loading emotion detection model (this may take a moment)...")
try:
    DeepFace.analyze(np.zeros((100, 100, 3)), actions=['emotion'], enforce_detection=False)
    print("Model loaded successfully.")
except Exception as e:
    print(f"Error pre-loading DeepFace model: {e}")
    # We can still try to proceed, it might load on the fly
    pass

# --- Initialize Webcam ---
cap = cv2.VideoCapture(0) # 0 is the default webcam

if not cap.isOpened():
    print("Error: Could not open webcam.")
    exit()

# --- Real-time Detection Loop ---
# We'll analyze emotions every N frames to improve performance,
# as emotion detection is computationally expensive.
FRAME_ANALYSIS_INTERVAL = 10 # Analyze every 10 frames
frame_counter = 0
# dominant_emotion = "Scanning..." # <-- REMOVED: This global variable caused the bug.

while True:
    # Read a frame from the webcam
    ret, frame = cap.read()
    if not ret:
        print("Error: Failed to capture frame.")
        break

    # Flip the frame horizontally (mirror effect)
    frame = cv2.flip(frame, 1)
    
    # Convert the frame to grayscale for the Haar cascade detector
    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

    # Detect faces in the grayscale frame
    faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))

    # Process each detected face
    for (x, y, w, h) in faces:
        # Draw a bounding box around the face
        cv2.rectangle(frame, (x, y), (x + w, y + h), BOX_COLOR, 2)
        
        # Only run emotion analysis every N frames
        if frame_counter % FRAME_ANALYSIS_INTERVAL == 0:
            try:
                # Extract the face Region of Interest (ROI)
                # DeepFace prefers BGR format, which 'frame' already is
                face_roi = frame[y:y + h, x:x + w]

                # Analyze the emotion of the face ROI
                # 'enforce_detection=False' tells deepface we've already found the face
                result = DeepFace.analyze(face_roi, actions=['emotion'], enforce_detection=False)
                
                # --- FIX 2: Check if result is valid before accessing ---
                if result and len(result) > 0:
                    # Get the dominant emotion for THIS face
                    raw_emotion = result[0]['dominant_emotion']
                    dominant_emotion = MOOD_MAP.get(raw_emotion, raw_emotion) # Use our map
                else:
                    dominant_emotion = "Unknown" # Handle case where analysis fails

                # --- FIX 1: Draw logic moved INSIDE the analysis block ---
                # This ensures the correct emotion is drawn on the correct face.
                
                # Put the text background
                (text_w, text_h), _ = cv2.getTextSize(dominant_emotion, FONT, 0.9, 2)
                cv2.rectangle(frame, (x, y - text_h - 15), (x + text_w, y - 10), BOX_COLOR, -1)
                # Put the text
                cv2.putText(frame, 
                            dominant_emotion, 
                            (x, y - 15), 
                            FONT, 
                            0.9, 
                            (0,0,0), # Black text for contrast on yellow bg
                            2)

            except Exception as e:
                # This can happen if the face is too small or at a weird angle
                # print(f"Analysis error: {e}")
                pass # Don't draw anything if an error occurs
        
        # --- Draw the emotion text --- (REMOVED from here)
        # The logic was moved up to fix the multi-face bug.

    # Increment frame counter
    frame_counter += 1

    # Display the resulting frame
    cv2.imshow('Mood Detector (Press \'q\' to quit)', frame)

    # Check for the 'q' key to exit the loop
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# --- Cleanup ---
cap.release()
cv2.destroyAllWindows()
print("Application closed.")

